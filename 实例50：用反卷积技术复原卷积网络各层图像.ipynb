{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例描述\n",
    "#### 将代码“cifar卷积”文件中的每层卷积结果进行反卷积并输出，通过tensorboard观察其结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10 import cifar10_input\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin\n",
      "begin data\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "print(\"begin\")\n",
    "images_train, labels_train = cifar10_input.inputs(eval_data=False, batch_size=batch_size)\n",
    "images_test, labels_test = cifar10_input.inputs(eval_data=True, batch_size=batch_size)\n",
    "print(\"begin data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大池化\n",
    "def max_pool_with_argmax(net, stride):\n",
    "    _, mask = tf.nn.max_pool_with_argmax( net,ksize=[1, stride, stride, 1], strides=[1, stride, stride, 1],padding='SAME')\n",
    "    mask = tf.stop_gradient(mask)\n",
    "    net = tf.nn.max_pool(net, ksize=[1, stride, stride, 1],strides=[1, stride, stride, 1], padding='SAME') \n",
    "    return net, mask\n",
    "#4*4----2*2--=2*2 【6，8，12，16】    \n",
    "#反池化\n",
    "def unpool(net, mask, stride):\n",
    "    ksize = [1, stride, stride, 1]\n",
    "    input_shape = net.get_shape().as_list()\n",
    "\n",
    "    output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n",
    "\n",
    "    one_like_mask = tf.ones_like(mask)\n",
    "    batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int64), shape=[input_shape[0], 1, 1, 1])\n",
    "    b = one_like_mask * batch_range\n",
    "    y = mask // (output_shape[2] * output_shape[3])\n",
    "    x = mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n",
    "    feature_range = tf.range(output_shape[3], dtype=tf.int64)\n",
    "    f = one_like_mask * feature_range\n",
    "\n",
    "    updates_size = tf.size(net)\n",
    "    indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n",
    "    values = tf.reshape(net, [updates_size])\n",
    "    ret = tf.scatter_nd(indices, values, output_shape)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "  \n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    " \n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')  \n",
    "       \n",
    "    \n",
    "def avg_pool_6x6(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 6, 6, 1],  strides=[1, 6, 6, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.替换Maxpool池化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 6, 6, 64)\n"
     ]
    }
   ],
   "source": [
    "# tf Graph Input\n",
    "x = tf.placeholder(tf.float32, [batch_size, 24,24,3]) # cifar data image of shape 24*24*3\n",
    "y = tf.placeholder(tf.float32, [batch_size, 10]) # 0-9 数字=> 10 classes\n",
    "\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 3, 64])\n",
    "b_conv1 = bias_variable([64])\n",
    "\n",
    "x_image = tf.reshape(x, [-1,24,24,3])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "#h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_pool1, mask1 = max_pool_with_argmax(h_conv1, 2)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 64, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "#h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "#############################################################\n",
    "h_pool2, mask = max_pool_with_argmax(h_conv2, 2)#(128, 6, 6, 64)\n",
    "print(h_pool2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.反卷积第二层卷积操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 12, 12, 64) (128, 12, 12, 64) (128, 12, 12, 64)\n"
     ]
    }
   ],
   "source": [
    "t_conv2 = unpool(h_pool2, mask, 2)#(128, 12, 12, 64)\n",
    "t_pool1 = tf.nn.conv2d_transpose(t_conv2-b_conv2, W_conv2, h_pool1.shape,[1,1,1,1])#(128, 24, 24, 64)\n",
    "print(t_conv2.shape,h_pool1.shape,t_pool1.shape)\n",
    "t_conv1 = unpool(t_pool1, mask1, 2)\n",
    "t_x_image = tf.nn.conv2d_transpose(t_conv1-b_conv1, W_conv1, x_image.shape,[1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.反卷积第一层卷积操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一层卷积还原\n",
    "t1_conv1 = unpool(h_pool1, mask1, 2)\n",
    "t1_x_image = tf.nn.conv2d_transpose(t1_conv1-b_conv1, W_conv1, x_image.shape,[1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.合并还原结果，并输出给TensorBoard输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成最终图像\n",
    "stitched_decodings = tf.concat((x_image, t1_x_image,t_x_image), axis=2)\n",
    "decoding_summary_op = tf.summary.image('source/cifar', stitched_decodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_conv3 = weight_variable([5, 5, 64, 10])\n",
    "b_conv3 = bias_variable([10])\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "\n",
    "nt_hpool3=avg_pool_6x6(h_conv3)#10\n",
    "nt_hpool3_flat = tf.reshape(nt_hpool3, [-1, 10])\n",
    "y_conv=tf.nn.softmax(nt_hpool3_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.session中写入log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.117188\n",
      "cross_entropy 1117.8351\n",
      "step 200, training accuracy 0.265625\n",
      "cross_entropy 703.5103\n",
      "step 400, training accuracy 0.429688\n",
      "cross_entropy 661.9038\n",
      "step 600, training accuracy 0.398438\n",
      "cross_entropy 633.505\n",
      "step 800, training accuracy 0.421875\n",
      "cross_entropy 600.52856\n",
      "step 1000, training accuracy 0.46875\n",
      "cross_entropy 583.8155\n",
      "step 1200, training accuracy 0.453125\n",
      "cross_entropy 565.0365\n",
      "step 1400, training accuracy 0.515625\n",
      "cross_entropy 536.87695\n",
      "step 1600, training accuracy 0.476562\n",
      "cross_entropy 524.3429\n",
      "step 1800, training accuracy 0.421875\n",
      "cross_entropy 520.216\n",
      "step 2000, training accuracy 0.46875\n",
      "cross_entropy 509.0088\n",
      "step 2200, training accuracy 0.421875\n",
      "cross_entropy 503.00696\n",
      "step 2400, training accuracy 0.546875\n",
      "cross_entropy 468.7069\n",
      "step 2600, training accuracy 0.476562\n",
      "cross_entropy 472.90756\n",
      "step 2800, training accuracy 0.53125\n",
      "cross_entropy 454.2824\n",
      "step 3000, training accuracy 0.507812\n",
      "cross_entropy 444.07568\n",
      "step 3200, training accuracy 0.609375\n",
      "cross_entropy 419.05573\n",
      "step 3400, training accuracy 0.453125\n",
      "cross_entropy 436.84625\n",
      "step 3600, training accuracy 0.523438\n",
      "cross_entropy 417.87067\n",
      "step 3800, training accuracy 0.585938\n",
      "cross_entropy 404.56476\n",
      "step 4000, training accuracy 0.554688\n",
      "cross_entropy 404.69672\n",
      "step 4200, training accuracy 0.539062\n",
      "cross_entropy 390.8375\n",
      "step 4400, training accuracy 0.585938\n",
      "cross_entropy 387.06116\n",
      "step 4600, training accuracy 0.585938\n",
      "cross_entropy 381.6187\n",
      "step 4800, training accuracy 0.640625\n",
      "cross_entropy 353.66058\n",
      "step 5000, training accuracy 0.507812\n",
      "cross_entropy 390.3325\n",
      "step 5200, training accuracy 0.515625\n",
      "cross_entropy 363.28705\n",
      "step 5400, training accuracy 0.601562\n",
      "cross_entropy 359.1872\n",
      "step 5600, training accuracy 0.515625\n",
      "cross_entropy 370.19394\n",
      "step 5800, training accuracy 0.601562\n",
      "cross_entropy 332.0575\n",
      "step 6000, training accuracy 0.523438\n",
      "cross_entropy 353.5151\n",
      "step 6200, training accuracy 0.515625\n",
      "cross_entropy 345.3841\n",
      "step 6400, training accuracy 0.59375\n",
      "cross_entropy 319.80124\n",
      "step 6600, training accuracy 0.640625\n",
      "cross_entropy 319.09207\n",
      "step 6800, training accuracy 0.585938\n",
      "cross_entropy 325.7672\n",
      "step 7000, training accuracy 0.5625\n",
      "cross_entropy 315.4178\n",
      "step 7200, training accuracy 0.585938\n",
      "cross_entropy 312.66193\n",
      "step 7400, training accuracy 0.515625\n",
      "cross_entropy 308.09637\n",
      "step 7600, training accuracy 0.507812\n",
      "cross_entropy 314.17786\n",
      "step 7800, training accuracy 0.554688\n",
      "cross_entropy 313.98962\n",
      "step 8000, training accuracy 0.632812\n",
      "cross_entropy 285.26587\n",
      "step 8200, training accuracy 0.632812\n",
      "cross_entropy 272.34048\n",
      "step 8400, training accuracy 0.570312\n",
      "cross_entropy 306.83105\n",
      "step 8600, training accuracy 0.554688\n",
      "cross_entropy 289.439\n",
      "step 8800, training accuracy 0.554688\n",
      "cross_entropy 284.6078\n",
      "step 9000, training accuracy 0.585938\n",
      "cross_entropy 284.1131\n",
      "step 9200, training accuracy 0.585938\n",
      "cross_entropy 273.70685\n",
      "step 9400, training accuracy 0.515625\n",
      "cross_entropy 290.20026\n",
      "step 9600, training accuracy 0.632812\n",
      "cross_entropy 268.97424\n",
      "step 9800, training accuracy 0.625\n",
      "cross_entropy 273.4474\n",
      "step 10000, training accuracy 0.59375\n",
      "cross_entropy 261.6849\n",
      "step 10200, training accuracy 0.664062\n",
      "cross_entropy 249.7679\n",
      "step 10400, training accuracy 0.59375\n",
      "cross_entropy 268.14316\n",
      "step 10600, training accuracy 0.664062\n",
      "cross_entropy 243.3012\n",
      "step 10800, training accuracy 0.632812\n",
      "cross_entropy 258.7065\n",
      "step 11000, training accuracy 0.59375\n",
      "cross_entropy 267.0996\n",
      "step 11200, training accuracy 0.578125\n",
      "cross_entropy 265.92673\n",
      "step 11400, training accuracy 0.734375\n",
      "cross_entropy 224.73557\n",
      "step 11600, training accuracy 0.703125\n",
      "cross_entropy 244.66797\n",
      "step 11800, training accuracy 0.625\n",
      "cross_entropy 237.39763\n",
      "step 12000, training accuracy 0.617188\n",
      "cross_entropy 233.30708\n",
      "step 12200, training accuracy 0.6875\n",
      "cross_entropy 225.80026\n",
      "step 12400, training accuracy 0.679688\n",
      "cross_entropy 234.63885\n",
      "step 12600, training accuracy 0.632812\n",
      "cross_entropy 222.35025\n",
      "step 12800, training accuracy 0.632812\n",
      "cross_entropy 225.32068\n",
      "step 13000, training accuracy 0.601562\n",
      "cross_entropy 232.83514\n",
      "step 13200, training accuracy 0.59375\n",
      "cross_entropy 232.21957\n",
      "step 13400, training accuracy 0.679688\n",
      "cross_entropy 211.59601\n",
      "step 13600, training accuracy 0.578125\n",
      "cross_entropy 229.28305\n",
      "step 13800, training accuracy 0.617188\n",
      "cross_entropy 232.44603\n",
      "step 14000, training accuracy 0.664062\n",
      "cross_entropy 216.24953\n",
      "step 14200, training accuracy 0.53125\n",
      "cross_entropy 259.7241\n",
      "step 14400, training accuracy 0.695312\n",
      "cross_entropy 212.36432\n",
      "step 14600, training accuracy 0.59375\n",
      "cross_entropy 222.87366\n",
      "step 14800, training accuracy 0.609375\n",
      "cross_entropy 219.49054\n",
      "finished！ test accuracy 0.695312\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = -tf.reduce_sum(y*tf.log(y_conv)) +(tf.nn.l2_loss(W_conv1)+tf.nn.l2_loss(W_conv2)+tf.nn.l2_loss(W_conv3))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "summary_writer = tf.summary.FileWriter('./log/cifar10_model', sess.graph)\n",
    "\n",
    "for i in range(15000):#20000\n",
    "    image_batch, label_batch = sess.run([images_train, labels_train])\n",
    "    label_b = np.eye(10,dtype=float)[label_batch] #one hot\n",
    "  \n",
    "    train_step.run(feed_dict={x:image_batch, y: label_b},session=sess)\n",
    "    #_, decoding_summary = sess.run([train_step, decoding_summary_op],feed_dict={x:image_batch, y: label_b})\n",
    "  \n",
    "    if i%200 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x:image_batch, y: label_b},session=sess)\n",
    "        print( \"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "        print(\"cross_entropy\",cross_entropy.eval(feed_dict={x:image_batch, y: label_b},session=sess))\n",
    "        #summary_writer.add_summary(decoding_summary)\n",
    "\n",
    "\n",
    "image_batch, label_batch = sess.run([images_test, labels_test])\n",
    "label_b = np.eye(10,dtype=float)[label_batch]#one hot\n",
    "print (\"finished！ test accuracy %g\"%accuracy.eval(feed_dict={\n",
    "     x:image_batch, y: label_b},session=sess))\n",
    "decoding_summary = sess.run(decoding_summary_op,feed_dict={x:image_batch, y: label_b})\n",
    "summary_writer.add_summary(decoding_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
